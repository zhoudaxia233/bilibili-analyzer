import subprocess
import os
import logging
import re
import tempfile
from pathlib import Path
from rich import print as rprint
from extract_cookies import get_bilibili_cookies


# Global cache for cookie files to avoid extracting them multiple times
_cookie_file_cache = {}
logger = logging.getLogger("bilibili_client")


def ensure_bilibili_url(identifier: str) -> str:
    """If identifier is a BVID, assemble the full Bilibili video URL."""
    if (identifier.startswith("BV") or identifier.startswith("bv")) and len(
        identifier
    ) >= 12:
        return f"https://www.bilibili.com/video/{identifier}"
    return identifier


def get_browser_cookies(browser: str) -> str:
    """Get cookie file for the specified browser, creating it if necessary.

    Args:
        browser: Browser name ('chrome' or 'firefox')

    Returns:
        Path to the cookie file
    """
    global _cookie_file_cache

    # If cookie file already exists in cache, return it
    if browser in _cookie_file_cache and os.path.exists(_cookie_file_cache[browser]):
        return _cookie_file_cache[browser]

    # Extract cookies from browser
    rprint(
        f"[cyan]Extracting Bilibili cookies from {browser} (this only happens once)...[/cyan]"
    )
    cookies = get_bilibili_cookies(browser)

    if not cookies:
        rprint(
            "[red]No Bilibili cookies found. Please ensure you are logged into Bilibili in your browser.[/red]"
        )
        return None

    # Create temporary cookie file
    cookie_file = tempfile.NamedTemporaryFile(delete=False, suffix=".cookies", mode="w")

    # Format cookies in Netscape format
    cookie_content = [
        "# Netscape HTTP Cookie File",
        "# https://curl.se/docs/http-cookies.html",
        "# This file was generated by Bilibili analyzer. Edit at your own risk.",
        "",
    ]

    # Add cookies in the required format
    domain = ".bilibili.com"
    for name, value in cookies.items():
        if name == "SESSDATA" and value:
            cookie_content.append(f"{domain}\tTRUE\t/\tTRUE\t0\tSESSDATA\t{value}")
        elif name == "bili_jct" and value:
            cookie_content.append(f"{domain}\tTRUE\t/\tTRUE\t0\tbili_jct\t{value}")
        elif name == "buvid3" and value:
            cookie_content.append(f"{domain}\tTRUE\t/\tTRUE\t0\tbuvid3\t{value}")

    # Write cookies to file
    cookie_file.write("\n".join(cookie_content))
    cookie_file.close()

    # Cache the cookie file path for future use
    _cookie_file_cache[browser] = cookie_file.name

    rprint(f"[green]Bilibili cookies extracted and saved for reuse.[/green]")
    return cookie_file.name


def download_with_ytdlp(
    url: str,
    output_path: str = None,
    download_type: str = "audio",
    credentials=None,  # Keeping parameter for backward compatibility
    browser=None,
):
    """Download media from a Bilibili video using yt-dlp.

    Args:
        url: Bilibili video URL
        output_path: Output path (optional)
        download_type: 'audio', 'subtitles', or 'all'
        credentials: Deprecated, use browser parameter instead
        browser: Browser to extract cookies from (e.g., 'chrome', 'firefox')
    """
    cmd = ["yt-dlp"]

    # Skip actual video/audio download if only subtitles are requested
    if download_type == "subtitles":
        cmd.append("--skip-download")

    # Add format specification based on download type
    if download_type in ["audio", "all"]:
        cmd.extend(["-f", "ba"])

    # Add subtitles option if requested
    if download_type in ["subtitles", "all"]:
        cmd.extend(["--write-subs", "--write-auto-subs", "--sub-langs", "all"])

    # Handle authentication - use cached cookie file for browser
    if browser:
        # Get cookie file from cache, extracting it only once if needed
        cookie_file = get_browser_cookies(browser)
        if cookie_file:
            cmd.extend(["--cookies", cookie_file])
            rprint(
                f"[cyan]Using cached cookies from {browser} browser for authentication[/cyan]"
            )
        else:
            rprint(
                f"[yellow]Warning: No cookies found for {browser}. Download may fail if authentication is required.[/yellow]"
            )
    elif credentials:
        # Legacy warning
        rprint(
            "[yellow]Warning: The credentials parameter is deprecated, please use --browser instead[/yellow]"
        )

    # Add URL and output template if specified
    cmd.append(url)
    if output_path:
        cmd.extend(["-o", output_path])

    # Add verbose output only in debug mode
    if logger.isEnabledFor(logging.DEBUG):
        cmd.append("-v")
        # Show command in debug mode only
        logger.debug(f"Running yt-dlp command: {' '.join(cmd)}")
    else:
        # Add quiet flag to reduce output when not in debug mode
        cmd.append("-q")

    # Run the command
    try:
        rprint(f"[cyan]Running yt-dlp command...[/cyan]")
        subprocess.run(cmd, check=True)

        # Provide more specific success message based on download type
        if download_type == "subtitles":
            rprint(f"[green]Subtitle download complete.[/green]")
        elif download_type == "audio":
            rprint(f"[green]Audio download complete.[/green]")
        else:
            rprint(f"[green]All requested content download complete.[/green]")
    except subprocess.CalledProcessError as e:
        # Provide more context in error messages to help debugging
        if download_type == "subtitles" and not browser:
            rprint(
                f"[red]yt-dlp subtitle download failed. You might need authentication with --browser.[/red]"
            )
        elif download_type == "audio" and not browser:
            rprint(
                f"[red]yt-dlp audio download failed. You might need authentication with --browser.[/red]"
            )
        else:
            rprint(f"[red]yt-dlp download failed with error code {e.returncode}[/red]")
        raise


def remove_timestamps(subtitle_text: str) -> str:
    """Remove timestamps from subtitles.

    Works with various subtitle formats:
    - Whisper output: [00:00.000 --> 00:02.880] Text
    - SRT format: 1\n00:00:00,000 --> 00:00:02,880\nText
    - VTT format: 00:00.000 --> 00:00:02.880\nText

    Args:
        subtitle_text: The subtitle text with timestamps

    Returns:
        Cleaned subtitle text with timestamps removed
    """
    # Different patterns to match various subtitle formats
    patterns = [
        # Whisper style [00:00.000 --> 00:02.880]
        r"\[\d{2}:\d{2}\.\d{3}\s*-->\s*\d{2}:\d{2}\.\d{3}\]\s*",
        # SRT style timestamps (with line numbers)
        r"^\d+\s*\n\d{2}:\d{2}:\d{2},\d{3}\s*-->\s*\d{2}:\d{2}:\d{2},\d{3}\s*\n",
        # VTT style timestamps
        r"^\d{2}:\d{2}[:.]\d{3}\s*-->\s*\d{2}:\d{2}[:.]\d{3}\s*\n",
        # Bilibili API style timestamps with from/to
        r"\[\d+\.\d+\]\s*",
    ]

    # Apply each pattern
    result = subtitle_text
    for pattern in patterns:
        result = re.sub(pattern, "", result, flags=re.MULTILINE)

    # Clean up multiple newlines
    result = re.sub(r"\n{3,}", "\n\n", result)

    return result.strip()


def format_subtitle_header(video_info, include_description=True) -> str:
    """Format a header with video information to prepend to subtitles.

    Args:
        video_info: VideoInfo object containing video metadata
        include_description: Whether to include video description

    Returns:
        Formatted header string
    """
    header = [
        f"# {video_info.title}",
        f"BVID: {video_info.bvid}",
        f"Uploader: {video_info.owner_name} (UID: {video_info.owner_mid})",
        f"Upload Time: {video_info.upload_time}",
        f"Views: {video_info.view_count:,}",
    ]

    if include_description and video_info.description:
        # Add a separator line
        header.append("\nDescription:")
        # Indent description lines
        description_lines = video_info.description.split("\n")
        for line in description_lines:
            header.append(f"> {line}")

    return "\n".join(header)
